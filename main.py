"""
main.py

Streamlit ê¸°ë°˜ ë‚œì†Œì•” ì¡°ì§ ì´ë¯¸ì§€ ë¶„ì„ ë° PubMed ìš”ì•½ UI ë©”ì¸ ì‹¤í–‰ íŒŒì¼

- ì´ë¯¸ì§€ ì—…ë¡œë“œ ê¸°ë°˜ ë¶„ì„ ê¸°ëŠ¥
- PubMed ê²€ìƒ‰ ë° AI ìš”ì•½ ê¸°ëŠ¥
- SessionStateë¥¼ í†µí•œ Q&A íˆìŠ¤í† ë¦¬ ê´€ë¦¬
- Attention Map ë° ì˜ˆì¸¡ ê²°ê³¼ í‘œì‹œ
"""

import streamlit as st
from PIL import Image
from io import BytesIO
import base64
import requests
import openai
import torch
import os
from openai import OpenAI
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import CharacterTextSplitter
from langchain_core.documents import Document
from Bio import Entrez
from dotenv import load_dotenv

load_dotenv()  # .env íŒŒì¼ì˜ í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°

OVIAN_TOKEN = os.environ.get("OVIAN_TOKEN")
OVIAN_IMAGE_KEY = os.environ.get("OVIAN_IMAGE_KEY")

Image.MAX_IMAGE_PIXELS = None

def get_safe_chat_model():
    return ChatOpenAI(
        model="gpt-4",
        temperature=0,
        api_key=st.secrets["OPENAI_API_KEY"]
    )

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "qa_history" not in st.session_state:
    st.session_state.qa_history = []
if "image_inferred" not in st.session_state:
    st.session_state.image_inferred = False

# LLM RAG í”„ë¡¬í”„íŠ¸
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ë‹¹ì‹ ì€ ë‚œì†Œì•” ë³‘ë¦¬ ì´ë¯¸ì§€ë¥¼ í•´ì„í•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ë…¼ë¬¸ì—ì„œ ë°œì·Œí•œ ë‚´ìš©ì…ë‹ˆë‹¤:
---------------------
{context}
---------------------

ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ì§ˆë¬¸: {question}
"""
)

# PubMed Boolean Query ìƒì„±
def generate_pubmed_query_from_question(question: str) -> str:
    openai.api_key = st.secrets["OPENAI_API_KEY"]

    prompt_str = f"""
ì•„ë˜ ì„¤ëª…ì€ ë³‘ë¦¬ ì´ë¯¸ì§€ AI ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.
ì´ ì„¤ëª…ì— ë§ëŠ” PubMed ê²€ìƒ‰ Boolean ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.

- í•µì‹¬ ì§ˆë³‘ëª…, ì¡°ì§ëª…, ì—¼ìƒ‰ë²•(H&E ë“±), ë³‘ë¦¬ ìš©ì–´ ì¤‘ì‹¬ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”.
        - Boolean ì—°ì‚°ì (AND, OR)ì™€ ê´„í˜¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
        - í‚¤ì›Œë“œëŠ” 3~6ê°œ ì‚¬ì´ë¡œ êµ¬ì„±í•˜ì„¸ìš”.
        - ì˜ˆì‹œ: (histology OR tissue) AND (H&E OR staining) AND (diagnosis OR cancer)

ì„¤ëª…:
{question}

ì¶œë ¥ í˜•ì‹: Boolean Query
"""

    client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë³‘ë¦¬ ì´ë¯¸ì§€ AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt_str}
        ],
        max_tokens=500
    )
    
    return response.choices[0].message.content.strip()


# PubMed ê²€ìƒ‰
def search_pubmed(question: str, max_results: int = 3):
    Entrez.email = "teamovianai@gmail.com"
    try:
        query = generate_pubmed_query_from_question(question)
        st.info(f"ğŸ” PubMed Boolean Query: {query}")
    except Exception as e:
        st.warning(f"âš ï¸ Boolean Query ìƒì„± ì‹¤íŒ¨ â†’ ì›ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰: {e}")
        query = question

    try:
        handle = Entrez.esearch(db="pubmed", term=query, retmax=max_results)
        record = Entrez.read(handle)
        ids = record.get("IdList", [])
        if not ids:
            st.warning("âŒ PubMedì—ì„œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        unique_summaries = {}  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš© (key: pmid, value: Document)
        for pmid in ids:
            # ì´ë¯¸ ì²˜ë¦¬í•œ pmidì´ë©´ ê±´ë„ˆëœë‹ˆë‹¤.
            if pmid in unique_summaries:
                continue
            
            try:
                summary = Entrez.esummary(db="pubmed", id=pmid, retmode="xml")
                summary_record = Entrez.read(summary)
                title = summary_record[0].get("Title", "[ì œëª© ì—†ìŒ]")
                pubdate = summary_record[0].get("PubDate", "")
                year = pubdate.split()[0] if pubdate else ""
                authors = summary_record[0].get("AuthorList", [])
                author_str = ", ".join(authors[:2]) + (" et al." if len(authors) > 2 else "")
                fetch = Entrez.efetch(db="pubmed", id=pmid, rettype="abstract", retmode="text")
                abstract = fetch.read()

                # Document ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€í•©ë‹ˆë‹¤.
                doc = Document(
                    page_content=abstract,
                    metadata={"pmid": pmid, "title": title, "year": year, "authors": author_str, "source": "pubmed"}
                )
                unique_summaries[pmid] = doc

            except Exception as e:
                st.warning(f"âš ï¸ PMID {pmid} ìš”ì•½ ì‹¤íŒ¨: {e}")
        
        # ë”•ì…”ë„ˆë¦¬ì˜ ê°’ë“¤(Document ê°ì²´)ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        return list(unique_summaries.values())
    
    except Exception as e:
        st.error(f"âŒ PubMed ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

# Streamlit UI ì‹œì‘
st.set_page_config(page_title="Ovarian Cancer RAG", layout="wide")
openai.api_key = st.secrets["OPENAI_API_KEY"]

st.title("ğŸ”¬ ë‚œì†Œì•” ë¶„ì„ AI ì–´ì‹œìŠ¤í„´íŠ¸")

uploaded_file = st.file_uploader("ì¡°ì§ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["png", "jpg", "jpeg"])

if st.button("ë¶„ì„ ì‹¤í–‰") and uploaded_file is not None:
    llm = get_safe_chat_model() 
    embeddings = HuggingFaceEmbeddings(model_name="dmis-lab/biobert-base-cased-v1.1")

    image_bytes = uploaded_file.read()

    try:
        with st.spinner("1ï¸âƒ£ Flask ì„œë²„ë¡œ ì´ë¯¸ì§€ ì „ì†¡ ë° AI ì¶”ë¡  ì¤‘..."):
            files = {'file': (uploaded_file.name, image_bytes)}
            headers = {"Authorization": f"Bearer {OVIAN_TOKEN}"}
            response = requests.post("http://localhost:5001/infer", files=files, headers=headers)

        if response.status_code == 200:
            result = response.json()
            pred_class = result["pred_class"]
            softmax_probs = result["softmax_probs"]
            max_prob = max(softmax_probs)
            attention_base64 = result["attention_map_base64"]
            
            # ë³´ì•ˆ ìƒíƒœ ë°°ì§€ (ì„œë²„ê°€ ë³´ë‚´ì¤€ security í”Œë˜ê·¸)
            sec = result.get("security", {})
            enc_ok = bool(sec.get("encrypt"))
            probe_ok = bool(sec.get("decrypt_probe"))
            dec_ok = bool(sec.get("decrypt"))

            st.markdown("### ğŸ” ë³´ì•ˆ ìƒíƒœ")
            st.write("âœ… ì•”í˜¸í™” ì €ì¥ ì„±ê³µ!" if enc_ok else "âŒ ì•”í˜¸í™” ì €ì¥ ì‹¤íŒ¨/ë¯¸ìˆ˜í–‰")
            st.write("âœ… ë³µí˜¸í™” ìœ íš¨ì„± ê²€ì‚¬ í†µê³¼!" if probe_ok else "âŒ ë³µí˜¸í™” ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨")
            st.write("âœ… ë³µí˜¸í™”(ì‹¤ì œ ì²˜ë¦¬ ìŠ¤íŠ¸ë¦¼) ì„±ê³µ!" if dec_ok else "âŒ ë³µí˜¸í™”(ì‹¤ì œ ì²˜ë¦¬ ìŠ¤íŠ¸ë¦¼) ì‹¤íŒ¨")
            st.markdown("---")

            st.session_state.image_inferred = True  # âœ… ì´ë¯¸ì§€ ì˜ˆì¸¡ ì™„ë£Œ í‘œì‹œ

            st.success("âœ… Flask ì„œë²„ ì¶”ë¡  ì™„ë£Œ!")

            # ì´ë¯¸ì§€ + Attention Map ë³‘ë ¬ í‘œì‹œ
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("ì›ë³¸ ì¡°ì§ ì´ë¯¸ì§€")
                st.image(Image.open(BytesIO(image_bytes)), use_container_width=True)
            with col2:
                st.subheader("AI Attention Map")
                st.image(BytesIO(base64.b64decode(attention_base64)), use_container_width=True)

            label_dict = {0: 'HGSC', 1: 'LGSC', 2: 'CC', 3: 'EC', 4: 'MC'}
            
            probs_percent = [f"{label_dict[i]}: {int(p * 100)}%" for i, p in enumerate(softmax_probs)]
            st.markdown("---")
            st.subheader("ğŸ“Š Softmax Probabilities")
            for line in probs_percent:
                st.write(f"- {line}")
            
            st.success(f"âœ… AI ì˜ˆì¸¡ í´ë˜ìŠ¤: {pred_class} ({label_dict[pred_class]})")

            # Softmax 35% ì´ìƒì¼ ë•Œë§Œ PubMed
            if max_prob >= 0.35:
                try:
                    with st.spinner("2ï¸âƒ£ PubMed ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘..."):
                        search_term = label_dict[pred_class] + " ovarian cancer"
                        related_papers = search_pubmed(search_term, max_results=3)
                        
                    
                    # AI ìš”ì•½ ë‹µë³€ í†µí•© ì¶œë ¥
                    st.markdown("---")
                    st.subheader("ğŸ§  AI ìš”ì•½ ë‹µë³€")

                    # ì „ì²´ ìš”ì•½
                    all_text = "\n\n".join([doc.page_content for doc in related_papers])
                    summary_prompt = f"""
                    You are a medical research summarization expert.

                    Please summarize the **overall trends and findings** from the following 3 papers about {label_dict[pred_class]} ovarian cancer.

                    âš ï¸ Do NOT summarize each document individually.  
                    Provide a unified 2â€“3 sentence overview only.

                    --- Documents ---
                    {all_text}
                    """
                    
                    client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
                    response = client.chat.completions.create(
                        model="gpt-4",
                        messages=[
                            {"role": "system", "content": "You are a medical paper summarization expert."},
                            {"role": "user", "content": summary_prompt}
                        ],
                        max_tokens=500
                    )
                    overall_summary = response.choices[0].message.content.strip()
                    st.markdown("**ì „ì²´ ìš”ì•½**")
                    st.markdown(f"- {overall_summary}")

                    # ë…¼ë¬¸ë³„ ìš”ì•½
                    st.markdown("**ë…¼ë¬¸ë³„ ìš”ì•½**")
                    for idx, doc in enumerate(related_papers, start=1):
                        title = doc.metadata.get("title", "[ì œëª© ì—†ìŒ]")

                        single_summary_prompt = f"""
                    Summarize this medical abstract in 3-4 sentences for a clinical researcher.

                    --- Abstract ---
                    {doc.page_content}
                    """
                        response = client.chat.completions.create(
                            model="gpt-4",
                            messages=[
                                {"role": "system", "content": "You are a medical paper summarization expert."},
                                {"role": "user", "content": single_summary_prompt}
                            ],
                            max_tokens=500
                        )
                        single_summary = response.choices[0].message.content.strip()

                        st.markdown(f"**{idx}. ì œëª©: {title}**")
                        st.markdown(f"- ìš”ì•½: {single_summary}")

                    # ê²°ë¡  ìš”ì•½
                    conclusion_prompt = f"""
                    Based on the previous summaries, write a single-sentence final conclusion about
                    {label_dict[pred_class]} ovarian cancer.
                    """
                    response = client.chat.completions.create(
                        model="gpt-4",
                        messages=[
                            {"role": "system", "content": "You are a medical summarizer."},
                            {"role": "user", "content": conclusion_prompt}
                        ],
                        max_tokens=200
                    )
                    conclusion = response.choices[0].message.content.strip()
                    st.markdown("**ê²°ë¡  ìš”ì•½**")
                    st.markdown(f"- {conclusion}")

                    # ê´€ë ¨ ë…¼ë¬¸ ì¶œë ¥
                    st.success("âœ… PubMed ê²€ìƒ‰ ì™„ë£Œ!")
                    st.markdown("---")
                    st.subheader("ğŸ“„ PubMed ê´€ë ¨ ë…¼ë¬¸")

                    for doc in related_papers:
                        title = doc.metadata.get("title", "[ì œëª© ì—†ìŒ]")
                        year = doc.metadata.get("year", "")
                        authors = doc.metadata.get("authors", "")
                        pmid = doc.metadata.get("pmid", "")
                        url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else ""
                        st.markdown(f"- [{title}]({url}) ({year}, {authors})" if url else f"- {title} ({year}, {authors})")

                except Exception as e:
                    st.warning(f"âš ï¸ PubMed ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            else:
                st.warning(f"âŒ ëª¨ë¸ í™•ì‹  ë¶€ì¡± (ìµœëŒ€ Softmax í™•ë¥  {int(max_prob*100)}%) â†’ PubMed ê²€ìƒ‰ ìƒëµ")
        else:
            st.error(f"âŒ Flask ì¶”ë¡  ì—ëŸ¬: {response.text}")
        # ì‹¤íŒ¨ ì‹œì—ë„ ì„œë²„ê°€ ë³´ë‚¸ security í”Œë˜ê·¸ê°€ ìˆìœ¼ë©´ í‘œì‹œ
            try:
                err = response.json()
                sec = err.get("security", {})
                if sec:
                    st.markdown("### ğŸ” ë³´ì•ˆ ìƒíƒœ(ì‹¤íŒ¨ ì‹œì )")
                    st.write("âœ… ì•”í˜¸í™” ì €ì¥ ì„±ê³µ!" if sec.get("encrypt") else "âŒ ì•”í˜¸í™” ì €ì¥ ì‹¤íŒ¨/ë¯¸ìˆ˜í–‰")
                    st.write("âœ… ë³µí˜¸í™” ìœ íš¨ì„± ê²€ì‚¬ í†µê³¼!" if sec.get("decrypt_probe") else "âŒ ë³µí˜¸í™” ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨")
                    st.write("âœ… ë³µí˜¸í™”(ì‹¤ì œ ì²˜ë¦¬ ìŠ¤íŠ¸ë¦¼) ì„±ê³µ!" if sec.get("decrypt") else "âŒ ë³µí˜¸í™”(ì‹¤ì œ ì²˜ë¦¬ ìŠ¤íŠ¸ë¦¼) ì‹¤íŒ¨")
                    st.markdown("---")
            except Exception:
                pass    
                    

    except Exception as e:
        st.error(f"ğŸš¨ Flask ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        st.markdown("### ğŸ” ë³´ì•ˆ ìƒíƒœ(ì—°ê²° ì‹¤íŒ¨)")
        st.write("âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨ë¡œ ë³´ì•ˆ ë‹¨ê³„ ì •ë³´ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.markdown("---")

    finally:
        try:
            headers = {"Authorization": f"Bearer {OVIAN_TOKEN}"}
            clear_response = requests.post("http://localhost:5001/clear_uploads", headers=headers)
            if clear_response.status_code == 200:
                st.success("âœ… ì„œë²„ uploads í´ë” ì •ë¦¬ ì™„ë£Œ!")
            else:
                st.warning(f"âš ï¸ uploads í´ë” ì •ë¦¬ ì‹¤íŒ¨: {clear_response.text}")
        except Exception as e:
            st.warning(f"âš ï¸ Flask ì„œë²„ ì •ë¦¬ ìš”ì²­ ì‹¤íŒ¨: {e}")


# main.py íŒŒì¼ì˜ ì¼ë¶€

# ì˜ˆì¸¡ ì´í›„ì—ë§Œ í…ìŠ¤íŠ¸ ì§ˆë¬¸ Q&A í™œì„±í™”
if st.session_state.image_inferred:
    st.markdown("---")
    st.subheader("ğŸ’¬ ì¶”ê°€ ì§ˆë¬¸ (í…ìŠ¤íŠ¸ ê¸°ë°˜ RAG Q&A)")

    # st.formìœ¼ë¡œ í…ìŠ¤íŠ¸ ì…ë ¥ê³¼ ì œì¶œ ë²„íŠ¼ì„ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
    with st.form(key="text_rag_form"):
        question = st.text_input(
            "AI ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•œ ì¶”ê°€ ì§ˆë¬¸ ì…ë ¥",
            placeholder="ì˜ˆì‹œ: What is the treatment for HGSC?",
            key="text_rag_input"
        )
        # st.button ëŒ€ì‹  st.form_submit_buttonì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        submit_button = st.form_submit_button(label="í…ìŠ¤íŠ¸ ê¸°ë°˜ Q&A ì‹¤í–‰")

        if submit_button:  # ë²„íŠ¼ì´ í´ë¦­ë˜ê±°ë‚˜, form ì•ˆì—ì„œ Enterë¥¼ ëˆ„ë¥´ë©´ Trueê°€ ë©ë‹ˆë‹¤.
            if not question.strip():
                st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                # ê¸°ì¡´ì˜ RAG ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥ ë¡œì§ì„ ì´ ì•ˆìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.
                with st.spinner("PubMed ê²€ìƒ‰ ë° AI ë‹µë³€ ìƒì„± ì¤‘..."):
                    llm = get_safe_chat_model()
                    embeddings = HuggingFaceEmbeddings(model_name="dmis-lab/biobert-base-cased-v1.1")

                    docs = search_pubmed(question=question)
                    if not docs:
                        st.warning("âŒ PubMedì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        splitter = CharacterTextSplitter(chunk_size=5000, chunk_overlap=200)
                        chunks = splitter.split_documents(docs)
                        vector_db = FAISS.from_documents(chunks, embeddings)
                        retriever = vector_db.as_retriever(search_kwargs={"k": 3})

                        rag_chain = RetrievalQA.from_chain_type(
                            llm=llm,
                            retriever=retriever,
                            chain_type="stuff",
                            chain_type_kwargs={"prompt": prompt_template},
                            return_source_documents=True
                        )
                        result = rag_chain.invoke({"query": question})

                        # ê²°ê³¼ ì¶œë ¥
                        st.markdown("---")
                        st.subheader("ğŸ–ï¸ ìš”ì•½ ë‹µë³€")

                        summary_prompt = f"""
                        You are a medical research summarization expert.

                        The user asked the following question:
                        "{question}"

                        You are given the content of 3 PubMed papers.  
                        Please answer using the following structure:

                        ğŸ’¬ Question Answer  
                        - Provide a clear 2â€“3 sentence answer to the user's question, based on the papers.

                        ğŸ“„ Related Paper Summaries  
                        1. Title: ...  
                        - Summary: ...

                        2. Title: ...  
                        - Summary: ...

                        3. Title: ...  
                        - Summary: ...

                        ğŸ”š Final Conclusion  
                        - In 1 sentence, summarize your answer to the question, based on the above papers.

                        --- Papers ---
                        """
                        for idx, doc in enumerate(result["source_documents"], start=1):
                            summary_prompt += f"\n\n{idx}. {doc.page_content}"

                        client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
                        response = client.chat.completions.create(
                            model="gpt-4",
                            messages=[
                                {"role": "system", "content": "You are a medical paper summarizer."},
                                {"role": "user", "content": summary_prompt}
                            ],
                            max_tokens=500
                        )
                        structured_summary = response.choices[0].message.content.strip()
                        
                         # 1. "Final Conclusion" ì²˜ë¦¬ (ë’¤ì— ìˆëŠ” ê²ƒë¶€í„° ì²˜ë¦¬í•´ì•¼ ìˆœì„œê°€ ì•ˆ ê¼¬ì…ë‹ˆë‹¤)
                        heading_fc = "Final Conclusion"
                        if heading_fc in structured_summary:
                            # "Final Conclusion" í…ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ì„ ë‚˜ëˆ•ë‹ˆë‹¤.
                            parts = structured_summary.split(heading_fc, 1)
                            # ì œëª© ì• ë¶€ë¶„(parts[0])ì—ì„œ í˜¹ì‹œ ìˆì„ ì´ëª¨ì§€ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
                            part_before = parts[0].replace("ğŸ”š", "").strip()
                            # ë‹µë³€ ë‚´ìš©(parts[1])ì—ì„œ í˜¹ì‹œ ìˆì„ ì½œë¡ (:)ì´ë‚˜ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤.
                            part_after = parts[1].lstrip(': ')
                            # ë‹¤ì‹œ ì¡°ë¦½í•©ë‹ˆë‹¤: (ì• ë‚´ìš©) + ì¤„ë°”ê¿ˆ + ì´ëª¨ì§€ + ì œëª© + ì¤„ë°”ê¿ˆ + ë‹µë³€
                            structured_summary = part_before + "\n\n" + "ğŸ”š " + heading_fc + "\n" + part_after

                        # 2. "Question Answer" ì²˜ë¦¬
                        heading_qa = "Question Answer"
                        if heading_qa in structured_summary:
                            parts = structured_summary.split(heading_qa, 1)
                            part_before = parts[0].replace("ğŸ’¬", "").strip()
                            part_after = parts[1].lstrip(': ')
                            structured_summary = part_before + "ğŸ’¬ " + heading_qa + "\n" + part_after
                            
                        st.markdown(structured_summary)

                        st.markdown("---")
                        st.subheader("ğŸ“„ ê´€ë ¨ ë…¼ë¬¸")
                        for doc in result["source_documents"]:
                            pmid = doc.metadata.get("pmid", "")
                            title = doc.metadata.get("title", "[ì œëª© ì—†ìŒ]")
                            year = doc.metadata.get("year", "")
                            authors = doc.metadata.get("authors", "")
                            url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else ""
                            st.markdown(f"- [{title}]({url}) ({year}, {authors})" if url else f"- {title} ({year}, {authors})")

                        st.session_state.qa_history.insert(0, {
                            "question": question,
                            "answer": structured_summary,
                            "sources": result["source_documents"]
                        })
            
else:
    st.info("ğŸ‘† ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  AI ì˜ˆì¸¡ì„ ì™„ë£Œí•˜ì„¸ìš”. ì´í›„ì— ì§ˆë¬¸ ê°€ëŠ¥!")

# Q&A íˆìŠ¤í† ë¦¬
if len(st.session_state.qa_history) > 0:
    st.markdown("## ğŸ“š ì´ì „ Q&A ê¸°ë¡")
    for idx, entry in enumerate(st.session_state.qa_history):
        with st.expander(f"Q{len(st.session_state.qa_history) - idx}: {entry['question']}"):
            st.write(entry["answer"])
            for doc in entry["sources"]:
                pmid = doc.metadata.get("pmid")
                title = doc.metadata.get("title", "[ì œëª© ì—†ìŒ]")
                year = doc.metadata.get("year", "")
                authors = doc.metadata.get("authors", "")
                url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else ""
                st.markdown(f"- [{title}]({url}) ({year}, {authors})" if url else f"- {title} ({year}, {authors})")

# ì´ˆê¸°í™” ë²„íŠ¼
if st.button("ê¸°ë¡ ì´ˆê¸°í™”"):
    st.session_state.qa_history = []
    st.session_state.image_inferred = False
    st.rerun()
