# -*- coding: utf-8 -*-
"""
utils.py

MIL(Multiple Instance Learning) ê¸°ë°˜ ë³‘ë¦¬ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ
ë°ì´í„° ë¡œë”©, ìƒ˜í”Œë§, ìµœì í™”, í‰ê°€ ë“± ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ ì •ì˜í•œ ëª¨ë“ˆì…ë‹ˆë‹¤.

ğŸ“Œ ì£¼ìš” ê¸°ëŠ¥:
- Custom Sampler ë° MIL ì „ìš© collate í•¨ìˆ˜ ì •ì˜
- í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ìš© DataLoader ìƒì„± (weighted/ìˆœì°¨/ë¬´ì‘ìœ„ ìƒ˜í”Œë§)
- ëª¨ë¸ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™” ë° êµ¬ì¡° ì¶œë ¥
- í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ìƒì„±
- MIL í•™ìŠµì„ ìœ„í•œ split ìƒì„±ê¸° ì œê³µ (train/val/test ë¶„ë¦¬)
- ì •í™•ë„ ë° ì˜¤ë¥˜ìœ¨ ê³„ì‚° í•¨ìˆ˜

ì´ ëª¨ë“ˆì€ AttriMIL ëª¨ë¸ í•™ìŠµ ì½”ë“œ(train_attrimil.py)ì™€ í•¨ê»˜ ì‚¬ìš©ë©ë‹ˆë‹¤.
"""

import pickle
import torch
import numpy as np
import torch.nn as nn
import pdb

import torch
import numpy as np
import torch.nn as nn
from torchvision import transforms
from torch.utils.data import DataLoader, Sampler, WeightedRandomSampler, RandomSampler, SequentialSampler, sampler
import torch.optim as optim
import pdb
import torch.nn.functional as F
import math
from itertools import islice
import collections

# GPU ì‚¬ìš© ì—¬ë¶€
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ì»¤ìŠ¤í…€ ìƒ˜í”ŒëŸ¬: ì£¼ì–´ì§„ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìˆœì°¨ì ìœ¼ë¡œ ìƒ˜í”Œë§
class SubsetSequentialSampler(Sampler):
    """
    ì£¼ì–´ì§„ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìƒ˜í”Œë§í•˜ëŠ” ì»¤ìŠ¤í…€ Sampler.
    """
    def __init__(self, indices):
        self.indices = indices

    def __iter__(self):
        return iter(self.indices)

    def __len__(self):
        return len(self.indices)

# MIL í•™ìŠµì„ ìœ„í•œ collate í•¨ìˆ˜ (ì¢Œí‘œ ì •ë³´ ì—†ì´ ì´ë¯¸ì§€ + ë¼ë²¨ë§Œ ë°˜í™˜)
def collate_MIL(batch):
    img = torch.cat([item[0] for item in batch], dim=0)
    label = torch.LongTensor([item[1] for item in batch])
    return [img, label]

# MIL í•™ìŠµì„ ìœ„í•œ collate í•¨ìˆ˜ (ì¢Œí‘œì™€ ìµœê·¼ì ‘ íŒ¨ì¹˜ ì •ë³´ í¬í•¨)
def collate_MIL_coords(batch):
    imgs = torch.cat([item[0] for item in batch], dim=0)
    labels = torch.LongTensor([item[1] for item in batch])
    coords = []
    nearest = []

    for item in batch:
        coord = item[2]
        near = item[3]

        if isinstance(coord, np.ndarray):
            coords.append(coord)
        else:
            coords.append(coord.cpu().numpy())

        if isinstance(near, np.ndarray):
            nearest.append(near)
        else:
            nearest.append(near.cpu().numpy())

    return imgs, labels, coords, nearest

# íŒ¨ì¹˜ ì¶”ë¡ ìš© collate í•¨ìˆ˜ (ì´ë¯¸ì§€ + ì¢Œí‘œë§Œ ë°˜í™˜)
def collate_features(batch):
    img = torch.cat([item[0] for item in batch], dim=0)
    coords = np.vstack([item[1] for item in batch])
    return [img, coords]

# ì¼ë°˜ì ì¸ ìˆœì°¨ DataLoader (Validation/Test ìš©ë„)
def get_simple_loader(dataset, batch_size=1, num_workers=1):
    kwargs = {'num_workers': 4, 'pin_memory': False, 'num_workers': num_workers} if device.type == "cuda" else {}
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler.SequentialSampler(dataset),
        collate_fn=collate_MIL,
        **kwargs
    )
    return loader 

# split_datasetì— ëŒ€í•´ í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ìš© DataLoader ìƒì„±
def get_split_loader(split_dataset, training=False, testing=False, weighted=False, batch_size=1):
    """
    split_datasetì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìƒí™©ì— ë”°ë¼ ì ì ˆí•œ DataLoader ë°˜í™˜
    """
    kwargs = {'num_workers': 4} if device.type == "cuda" else {}

    if not testing:
        if training:
            if weighted:
                weights = make_weights_for_balanced_classes_split(split_dataset)
                loader = DataLoader(
                    split_dataset,
                    batch_size=batch_size,
                    sampler=WeightedRandomSampler(weights, len(weights)),
                    collate_fn=collate_MIL_coords,
                    **kwargs
                )
            else:
                loader = DataLoader(
                    split_dataset,
                    batch_size=batch_size,
                    sampler=RandomSampler(split_dataset),
                    collate_fn=collate_MIL_coords,
                    **kwargs
                )
        else:
            loader = DataLoader(
                split_dataset,
                batch_size=batch_size,
                sampler=SequentialSampler(split_dataset),
                collate_fn=collate_MIL_coords,
                **kwargs
            )
    else:
        ids = np.random.choice(np.arange(len(split_dataset)), int(len(split_dataset) * 0.1), replace=False)
        loader = DataLoader(
            split_dataset,
            batch_size=batch_size,
            sampler=SubsetSequentialSampler(ids),
            collate_fn=collate_MIL,
            **kwargs
        )

    return loader

# í•™ìŠµ optimizer ìƒì„±
def get_optim(model, args):
    if args.opt == "adam":
        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=args.reg)
    elif args.opt == 'sgd':
        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=0.9, weight_decay=args.reg)
    else:
        raise NotImplementedError
    return optimizer

# ëª¨ë¸ êµ¬ì¡° ë° íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
def print_network(net):
    num_params = 0
    num_params_train = 0
    print(net)
    
    for param in net.parameters():
        n = param.numel()
        num_params += n
        if param.requires_grad:
            num_params_train += n
    
    print('Total number of parameters: %d' % num_params)
    print('Total number of trainable parameters: %d' % num_params_train)

# MILì—ì„œ í´ë˜ìŠ¤ë³„ split ìƒì„±ê¸° (train/val/test ë¶„ë¦¬)
def generate_split(cls_ids, val_num, test_num, samples, n_splits=5,
                   seed=7, label_frac=1.0, custom_test_ids=None):
    indices = np.arange(samples).astype(int)
    
    if custom_test_ids is not None:
        indices = np.setdiff1d(indices, custom_test_ids)

    np.random.seed(seed)
    for i in range(n_splits):
        all_val_ids = []
        all_test_ids = []
        sampled_train_ids = []
        
        if custom_test_ids is not None:
            all_test_ids.extend(custom_test_ids)

        for c in range(len(val_num)):
            possible_indices = np.intersect1d(cls_ids[c], indices)
            val_ids = np.random.choice(possible_indices, val_num[c], replace=False)
            remaining_ids = np.setdiff1d(possible_indices, val_ids)
            all_val_ids.extend(val_ids)

            if custom_test_ids is None:
                test_ids = np.random.choice(remaining_ids, test_num[c], replace=False)
                remaining_ids = np.setdiff1d(remaining_ids, test_ids)
                all_test_ids.extend(test_ids)

            if label_frac == 1:
                sampled_train_ids.extend(remaining_ids)
            else:
                sample_num = math.ceil(len(remaining_ids) * label_frac)
                slice_ids = np.arange(sample_num)
                sampled_train_ids.extend(remaining_ids[slice_ids])

        yield sampled_train_ids, all_val_ids, all_test_ids

# iterableì—ì„œ në²ˆì§¸ ìš”ì†Œ ê°€ì ¸ì˜¤ê¸°
def nth(iterator, n, default=None):
    if n is None:
        return collections.deque(iterator, maxlen=0)
    else:
        return next(islice(iterator, n, None), default)

# ì˜ˆì¸¡ ì˜¤ë¥˜ìœ¨ ê³„ì‚° (1 - accuracy)
def calculate_error(Y_hat, Y):
    error = 1. - Y_hat.float().eq(Y.float()).float().mean().item()
    return error

# í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì •ìš© ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ê³„ì‚° (trainìš©)
def make_weights_for_balanced_classes_split(dataset):
    N = float(len(dataset))                                           
    weight_per_class = [N / len(dataset.slide_cls_ids[c]) for c in range(len(dataset.slide_cls_ids))]                                                                                                     
    weight = [0] * int(N)                                           
    for idx in range(len(dataset)):   
        y = dataset.getlabel(idx)                        
        weight[idx] = weight_per_class[y]                                  
    return torch.DoubleTensor(weight)

# ëª¨ë¸ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™” (Linear, BatchNorm ê³„ì¸µ ëŒ€ìƒ)
def initialize_weights(module):
    for m in module.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight)
            m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm1d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
