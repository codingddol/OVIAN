# -*- coding: utf-8 -*-
"""
AttriMIL.py

AttriMIL (Attribute-based Multiple Instance Learning) ëª¨ë¸ êµ¬í˜„ íŒŒì¼ì…ë‹ˆë‹¤.
ë³‘ë¦¬ ì´ë¯¸ì§€ ë¶„ì„ì—ì„œ MIL ë°©ì‹ìœ¼ë¡œ ì•„í˜• ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ë©°, í´ë˜ìŠ¤ë³„ë¡œ ë…ë¦½ì ì¸
Attention ë„¤íŠ¸ì›Œí¬ì™€ Classifierë¥¼ êµ¬ì„±í•˜ì—¬ ê°€ì¤‘ëœ instance-level ì •ë³´ ì§‘ê³„ ë°©ì‹ìœ¼ë¡œ
bag-level ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ğŸ“Œ ì£¼ìš” êµ¬ì„±:
- Attn_Net_Gated: í´ë˜ìŠ¤ë³„ attention ì ìˆ˜ë¥¼ ì‚°ì¶œí•˜ëŠ” Gated Attention ëª¨ë“ˆ
- AttriMIL: í´ë˜ìŠ¤ë³„ attention + ë¶„ë¥˜ê¸°ë¥¼ í†µí•´ í•´ì„ ê°€ëŠ¥í•œ MIL êµ¬ì¡° êµ¬í˜„

ì¶œë ¥:
- logits: í´ë˜ìŠ¤ë³„ logit ì ìˆ˜
- Y_prob: softmaxëœ í´ë˜ìŠ¤ë³„ í™•ë¥ 
- Y_hat: ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ (argmax)
- attribute_score: exp(attn) x instance_scoreë¡œ ê³„ì‚°ëœ ì£¼ëª© ì ìˆ˜
- results_dict: (í˜„ì¬ ë¯¸ì‚¬ìš©, í™•ì¥ ê°€ëŠ¥)

ì´ ëª¨ë¸ì€ train_attrimil.pyì—ì„œ í•™ìŠµ ë£¨í”„ì™€ í•¨ê»˜ ì‚¬ìš©ë©ë‹ˆë‹¤.
"""


import torch
import torch.nn as nn
import torch.nn.functional as F

# í´ë˜ìŠ¤ë³„ Gated Attention ì ìˆ˜ë¥¼ ì‚°ì¶œí•˜ëŠ” ì„œë¸Œ ë„¤íŠ¸ì›Œí¬
class Attn_Net_Gated(nn.Module):
    def __init__(self, L=1024, D=256, dropout=False, n_classes=1):
        super(Attn_Net_Gated, self).__init__()
        # Gated Attention êµ¬ì„±: tanhì™€ sigmoid ê°ê° í†µê³¼
        self.attention_a = [nn.Linear(L, D), nn.Tanh()]
        self.attention_b = [nn.Linear(L, D), nn.Sigmoid()]
        if dropout:
            self.attention_a.append(nn.Dropout(0.25))
            self.attention_b.append(nn.Dropout(0.25))

        # Sequentialë¡œ ë¬¶ê¸°
        self.attention_a = nn.Sequential(*self.attention_a)
        self.attention_b = nn.Sequential(*self.attention_b)
        self.attention_c = nn.Linear(D, n_classes)  # ìµœì¢… attention score ì¶œë ¥

    def forward(self, x):
        a = self.attention_a(x)
        b = self.attention_b(x)
        A = a.mul(b)  # gated attention
        A = self.attention_c(A)  # shape: (N, n_classes)
        return A, x


# AttriMIL ëª¨ë¸ ì •ì˜
class AttriMIL(nn.Module):
    def __init__(self, n_classes=2, dim=512):
        super().__init__()
        # ì…ë ¥ feature ë³´ì •ìš© residual adaptor
        self.adaptor = nn.Sequential(
            nn.Linear(dim, dim // 2),
            nn.ReLU(),
            nn.Linear(dim // 2, dim)
        )

        # í´ë˜ìŠ¤ë³„ attention network ë¦¬ìŠ¤íŠ¸
        self.attention_nets = nn.ModuleList([
            Attn_Net_Gated(L=dim, D=dim // 2) for _ in range(n_classes)
        ])

        # í´ë˜ìŠ¤ë³„ classifier ë¦¬ìŠ¤íŠ¸ (instance score ìƒì„±)
        self.classifiers = nn.ModuleList([
            nn.Linear(dim, 1) for _ in range(n_classes)
        ])

        self.n_classes = n_classes
        self.bias = nn.Parameter(torch.zeros(n_classes), requires_grad=True)  # í´ë˜ìŠ¤ë³„ bias í•™ìŠµ ê°€ëŠ¥

    def forward(self, h):
        device = h.device
        h = h + self.adaptor(h)  # residual adaptor ì ìš©

        # í´ë˜ìŠ¤ë³„ raw attention ì €ì¥ (shape: [C, N])
        A_raw = torch.empty(self.n_classes, h.size(0), device=device)

        # í´ë˜ìŠ¤ë³„ instance score ì €ì¥ (shape: [1, C, N])
        instance_score = torch.empty(1, self.n_classes, h.size(0), device=device)

        # í´ë˜ìŠ¤ë³„ attentionê³¼ instance score ê³„ì‚°
        for c in range(self.n_classes):
            A, h_out = self.attention_nets[c](h)  # (N, 1), (N, dim)
            A = A.view(-1)  # (N,) í˜•íƒœë¡œ ë³€í™˜
            A_raw[c, :] = A  # attention ì €ì¥
            instance_score[0, c, :] = self.classifiers[c](h_out).squeeze(-1)  # instance score ì €ì¥

        # attribute score = instance score Ã— exp(attention)
        attribute_score = torch.empty(1, self.n_classes, h.size(0), device=device)
        for c in range(self.n_classes):
            attribute_score[0, c, :] = instance_score[0, c, :] * torch.exp(A_raw[c, :])

        # í´ë˜ìŠ¤ë³„ bag-level logit ê³„ì‚°
        logits = torch.empty(1, self.n_classes, device=device)
        for c in range(self.n_classes):
            logits[0, c] = (
                torch.sum(attribute_score[0, c, :], dim=-1) /
                torch.sum(torch.exp(A_raw[c, :]), dim=-1)
            ) + self.bias[c]  # bias ì¶”ê°€

        # ìµœì¢… ì¶œë ¥ê°’
        Y_hat = torch.topk(logits, 1, dim=1)[1]  # ì˜ˆì¸¡ í´ë˜ìŠ¤
        Y_prob = F.softmax(logits, dim=1)       # softmax í™•ë¥ 
        results_dict = {}  # í™•ì¥ ê°€ëŠ¥ (í˜„ì¬ ë¯¸ì‚¬ìš©)

        return logits, Y_prob, Y_hat, attribute_score, results_dict
